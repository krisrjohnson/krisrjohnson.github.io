---
layout: single
title: Stanford cs231 KNN
---

## KNN Image Classifier, Standford [cs231](http://cs231n.github.io/classification/)
Good pic explaining difficulties of attempting to hard code an image object classifier:
![Image Variations](/assets/images/img_variations.jpeg "yao-za ;)")

Creating a k-Nearest Neighbors classifier. Very straightforward. KNNs are data-driven, meaning acquire data then train instead versus explicitly coding first.

Goal: given 10 classes and 5k images per class, want to determine the class of a new image.

kNN: Split data into train, validate, and test data. With large datasets (\~1mm+) that'd turn into 98%, 1%, 1%. Flatten each pic, and put all of train data into memory. With the validation data, iterate over potential values for k and return accuracy (total right / total possible).

For each picture, you predict it's class with the kNN by comparing to every pic in the training set, doing least absolute deviation pixel by pixel, channel by channel (RGB) and summing. Then arg-max smallest k LADs to return class, could L2 but it's more affected by outliers.

Pros:
1. Training time is non-existent, only choosing hyperparam k takes time
1. Very fast first pass - can subsample from the training set if data too large to hold in memory
1. Accurate on low dimension data - which img data is not

Cons:
1. Computationally heavy, requires all of training data to be held in memory to perform nearest neighbor, in this case by LADs pixel to pixel distance across all pics
1. Very simple

In numpy code:
```python
Xtr, Ytr, Xte, Yte = Cifar10 #40k,32,323; 40k; 10k,32,32,3; 10k

class NearestNeighbor():
  __init__():
    pass

  def train(self, X, Y):
    self.Xtr = X
    self.Ytr = Y
    #something else for the Y

  def predict(self, k=1, Xte):
    #Xte of shape NxD, Yte shape N
    num_test = X.shape[0]
    Y_pred = np.zeros(num_test, dtype=self.ytr.dtype)

    for i in xrange(num_test):
      dist = np.sum(np.abs(self.Xtr-X[i,:]), axis=1) #X[i,:] broadcasts to Xtr.shape
      min_index = np.argmin(dist)
      Y_pred[i] = self.Ytr[min_index]

    return Y_pred


Xtr_rows = Xtr.reshape(Xtr.shape[0], 32*32*3) #why not .view()?
Xte_rows = Xte.reshape(Xte.shape[0], 32*32*3)

nn = NearestNeighbor()
nn.train(Xtr_rows, Ytr)
Yte_pred = nn.predict(Xte)
print(f'accuracy: {np.mean(Yte_pred == Yte)}')
```

### Cross Validation
![Cross Validation](/assets/images/cross-validation.jpeg "yao-za ;)")

Since we only ever want to run against test data once, to do hyperparam tuning we use validation data. For small datasets, we do k-fold cross validation, which involves creating k splices, folds, of the data. Then iterating over every fold as the validation set and running the model all the way through to get accuracy. So for 5 folds, each fold would play validation set once, and you'd end up with 5 accuracy metrics. Averaging, you'd get your average accuracy for that fold.

Obviously this multiplies directly against computation time.
